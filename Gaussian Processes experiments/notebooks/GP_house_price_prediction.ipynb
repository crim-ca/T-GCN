{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP regression house price prediction\n",
    "\n",
    "\n",
    "Objectif: Utiliser une regresison Gaussienne pour prédire le prix d'une maison. On peut également obtenir un intervale de confiance à 2 sigma pour la prediction.\n",
    "\n",
    "\n",
    "Utilisation du Dataset UCI Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CRIM: Per capita crime rate by town\n",
    "* ZN: Proportion of residential land zoned for lots over 25,000 sq. ft\n",
    "* INDUS: Proportion of non-retail business acres per town\n",
    "* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* NOX: Nitric oxide concentration (parts per 10 million)\n",
    "* RM: Average number of rooms per dwelling\n",
    "* AGE: Proportion of owner-occupied units built prior to 1940\n",
    "* DIS: Weighted distances to five Boston employment centers\n",
    "* RAD: Index of accessibility to radial highways\n",
    "* TAX: Full-value property tax rate per \\$10,000\n",
    "* PTRATIO: Pupil-teacher ratio by town\n",
    "* $ B: 1000(Bk — 0.63)^2 $, where Bk is the proportion of [people of African American descent] by town\n",
    "* LSTAT: Percentage of lower status of the population\n",
    "* MEDV: Median value of owner-occupied homes in $1000s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "We use the UCI boston hosuing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "boston = pd.DataFrame(load_boston().data, columns=load_boston().feature_names)\n",
    "boston['MEDV'] = load_boston().target\n",
    "boston.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([404, 2])\n",
      "torch.Size([102, 2])\n",
      "torch.Size([404])\n",
      "torch.Size([102])\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(np.c_[boston['LSTAT'], boston['RM']], columns = ['LSTAT','RM'])\n",
    "Y = boston['MEDV']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
    "X_train = torch.tensor(X_train.values).to(device)\n",
    "Y_train = torch.tensor(Y_train.values).to(device)\n",
    "X_test = torch.tensor(X_test.values).to(device)\n",
    "Y_test = torch.tensor(Y_test.values).to(device)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "We use a GP woth RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().double().to(device)\n",
    "model = ExactGPModel(X_train, Y_train, likelihood).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/2000 - Loss: 40.323   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/2000 - Loss: 15.573   lengthscale: 1.245   noise: 1.278\n",
      "Iter 21/2000 - Loss: 9.980   lengthscale: 1.700   noise: 1.888\n",
      "Iter 31/2000 - Loss: 7.993   lengthscale: 1.991   noise: 2.403\n",
      "Iter 41/2000 - Loss: 7.039   lengthscale: 2.171   noise: 2.815\n",
      "Iter 51/2000 - Loss: 6.477   lengthscale: 2.289   noise: 3.149\n",
      "Iter 61/2000 - Loss: 6.099   lengthscale: 2.373   noise: 3.431\n",
      "Iter 71/2000 - Loss: 5.811   lengthscale: 2.439   noise: 3.681\n",
      "Iter 81/2000 - Loss: 5.587   lengthscale: 2.494   noise: 3.907\n",
      "Iter 91/2000 - Loss: 5.400   lengthscale: 2.542   noise: 4.117\n",
      "Iter 101/2000 - Loss: 5.241   lengthscale: 2.584   noise: 4.314\n",
      "Iter 111/2000 - Loss: 5.100   lengthscale: 2.623   noise: 4.501\n",
      "Iter 121/2000 - Loss: 4.981   lengthscale: 2.657   noise: 4.678\n",
      "Iter 131/2000 - Loss: 4.872   lengthscale: 2.688   noise: 4.848\n",
      "Iter 141/2000 - Loss: 4.778   lengthscale: 2.717   noise: 5.011\n",
      "Iter 151/2000 - Loss: 4.691   lengthscale: 2.743   noise: 5.168\n",
      "Iter 161/2000 - Loss: 4.613   lengthscale: 2.767   noise: 5.320\n",
      "Iter 171/2000 - Loss: 4.541   lengthscale: 2.788   noise: 5.466\n",
      "Iter 181/2000 - Loss: 4.479   lengthscale: 2.808   noise: 5.608\n",
      "Iter 191/2000 - Loss: 4.417   lengthscale: 2.826   noise: 5.746\n",
      "Iter 201/2000 - Loss: 4.359   lengthscale: 2.842   noise: 5.880\n",
      "Iter 211/2000 - Loss: 4.309   lengthscale: 2.857   noise: 6.011\n",
      "Iter 221/2000 - Loss: 4.262   lengthscale: 2.871   noise: 6.138\n",
      "Iter 231/2000 - Loss: 4.217   lengthscale: 2.883   noise: 6.262\n",
      "Iter 241/2000 - Loss: 4.173   lengthscale: 2.894   noise: 6.383\n",
      "Iter 251/2000 - Loss: 4.133   lengthscale: 2.903   noise: 6.501\n",
      "Iter 261/2000 - Loss: 4.099   lengthscale: 2.911   noise: 6.616\n",
      "Iter 271/2000 - Loss: 4.064   lengthscale: 2.919   noise: 6.729\n",
      "Iter 281/2000 - Loss: 4.031   lengthscale: 2.925   noise: 6.839\n",
      "Iter 291/2000 - Loss: 4.001   lengthscale: 2.931   noise: 6.948\n",
      "Iter 301/2000 - Loss: 3.969   lengthscale: 2.935   noise: 7.054\n",
      "Iter 311/2000 - Loss: 3.942   lengthscale: 2.939   noise: 7.159\n",
      "Iter 321/2000 - Loss: 3.915   lengthscale: 2.942   noise: 7.261\n",
      "Iter 331/2000 - Loss: 3.891   lengthscale: 2.944   noise: 7.362\n",
      "Iter 341/2000 - Loss: 3.866   lengthscale: 2.945   noise: 7.460\n",
      "Iter 351/2000 - Loss: 3.844   lengthscale: 2.946   noise: 7.558\n",
      "Iter 361/2000 - Loss: 3.820   lengthscale: 2.946   noise: 7.653\n",
      "Iter 371/2000 - Loss: 3.799   lengthscale: 2.946   noise: 7.747\n",
      "Iter 381/2000 - Loss: 3.778   lengthscale: 2.944   noise: 7.839\n",
      "Iter 391/2000 - Loss: 3.760   lengthscale: 2.942   noise: 7.930\n",
      "Iter 401/2000 - Loss: 3.742   lengthscale: 2.940   noise: 8.020\n",
      "Iter 411/2000 - Loss: 3.723   lengthscale: 2.937   noise: 8.108\n",
      "Iter 421/2000 - Loss: 3.706   lengthscale: 2.933   noise: 8.195\n",
      "Iter 431/2000 - Loss: 3.691   lengthscale: 2.929   noise: 8.280\n",
      "Iter 441/2000 - Loss: 3.673   lengthscale: 2.924   noise: 8.365\n",
      "Iter 451/2000 - Loss: 3.659   lengthscale: 2.919   noise: 8.448\n",
      "Iter 461/2000 - Loss: 3.643   lengthscale: 2.913   noise: 8.530\n",
      "Iter 471/2000 - Loss: 3.629   lengthscale: 2.906   noise: 8.612\n",
      "Iter 481/2000 - Loss: 3.615   lengthscale: 2.900   noise: 8.692\n",
      "Iter 491/2000 - Loss: 3.602   lengthscale: 2.893   noise: 8.771\n",
      "Iter 501/2000 - Loss: 3.589   lengthscale: 2.885   noise: 8.849\n",
      "Iter 511/2000 - Loss: 3.577   lengthscale: 2.877   noise: 8.926\n",
      "Iter 521/2000 - Loss: 3.566   lengthscale: 2.869   noise: 9.002\n",
      "Iter 531/2000 - Loss: 3.553   lengthscale: 2.860   noise: 9.077\n",
      "Iter 541/2000 - Loss: 3.543   lengthscale: 2.851   noise: 9.151\n",
      "Iter 551/2000 - Loss: 3.531   lengthscale: 2.842   noise: 9.225\n",
      "Iter 561/2000 - Loss: 3.521   lengthscale: 2.833   noise: 9.297\n",
      "Iter 571/2000 - Loss: 3.511   lengthscale: 2.823   noise: 9.368\n",
      "Iter 581/2000 - Loss: 3.498   lengthscale: 2.813   noise: 9.439\n",
      "Iter 591/2000 - Loss: 3.491   lengthscale: 2.802   noise: 9.509\n",
      "Iter 601/2000 - Loss: 3.481   lengthscale: 2.792   noise: 9.577\n",
      "Iter 611/2000 - Loss: 3.473   lengthscale: 2.781   noise: 9.646\n",
      "Iter 621/2000 - Loss: 3.463   lengthscale: 2.770   noise: 9.714\n",
      "Iter 631/2000 - Loss: 3.454   lengthscale: 2.759   noise: 9.781\n",
      "Iter 641/2000 - Loss: 3.444   lengthscale: 2.748   noise: 9.848\n",
      "Iter 651/2000 - Loss: 3.437   lengthscale: 2.736   noise: 9.913\n",
      "Iter 661/2000 - Loss: 3.428   lengthscale: 2.724   noise: 9.978\n",
      "Iter 671/2000 - Loss: 3.420   lengthscale: 2.713   noise: 10.042\n",
      "Iter 681/2000 - Loss: 3.416   lengthscale: 2.701   noise: 10.106\n",
      "Iter 691/2000 - Loss: 3.409   lengthscale: 2.689   noise: 10.169\n",
      "Iter 701/2000 - Loss: 3.400   lengthscale: 2.677   noise: 10.231\n",
      "Iter 711/2000 - Loss: 3.393   lengthscale: 2.665   noise: 10.292\n",
      "Iter 721/2000 - Loss: 3.384   lengthscale: 2.653   noise: 10.353\n",
      "Iter 731/2000 - Loss: 3.378   lengthscale: 2.642   noise: 10.414\n",
      "Iter 741/2000 - Loss: 3.372   lengthscale: 2.630   noise: 10.473\n",
      "Iter 751/2000 - Loss: 3.366   lengthscale: 2.619   noise: 10.532\n",
      "Iter 761/2000 - Loss: 3.364   lengthscale: 2.607   noise: 10.591\n",
      "Iter 771/2000 - Loss: 3.355   lengthscale: 2.595   noise: 10.649\n",
      "Iter 781/2000 - Loss: 3.348   lengthscale: 2.584   noise: 10.706\n",
      "Iter 791/2000 - Loss: 3.343   lengthscale: 2.572   noise: 10.764\n",
      "Iter 801/2000 - Loss: 3.339   lengthscale: 2.561   noise: 10.820\n",
      "Iter 811/2000 - Loss: 3.332   lengthscale: 2.550   noise: 10.877\n",
      "Iter 821/2000 - Loss: 3.327   lengthscale: 2.539   noise: 10.932\n",
      "Iter 831/2000 - Loss: 3.320   lengthscale: 2.528   noise: 10.988\n",
      "Iter 841/2000 - Loss: 3.319   lengthscale: 2.517   noise: 11.042\n",
      "Iter 851/2000 - Loss: 3.313   lengthscale: 2.507   noise: 11.097\n",
      "Iter 861/2000 - Loss: 3.310   lengthscale: 2.496   noise: 11.150\n",
      "Iter 871/2000 - Loss: 3.302   lengthscale: 2.485   noise: 11.204\n",
      "Iter 881/2000 - Loss: 3.297   lengthscale: 2.474   noise: 11.257\n",
      "Iter 891/2000 - Loss: 3.292   lengthscale: 2.464   noise: 11.309\n",
      "Iter 901/2000 - Loss: 3.290   lengthscale: 2.453   noise: 11.362\n",
      "Iter 911/2000 - Loss: 3.285   lengthscale: 2.443   noise: 11.414\n",
      "Iter 921/2000 - Loss: 3.283   lengthscale: 2.433   noise: 11.465\n",
      "Iter 931/2000 - Loss: 3.277   lengthscale: 2.424   noise: 11.516\n",
      "Iter 941/2000 - Loss: 3.275   lengthscale: 2.414   noise: 11.567\n",
      "Iter 951/2000 - Loss: 3.273   lengthscale: 2.405   noise: 11.617\n",
      "Iter 961/2000 - Loss: 3.267   lengthscale: 2.396   noise: 11.667\n",
      "Iter 971/2000 - Loss: 3.262   lengthscale: 2.387   noise: 11.716\n",
      "Iter 981/2000 - Loss: 3.265   lengthscale: 2.379   noise: 11.765\n",
      "Iter 991/2000 - Loss: 3.256   lengthscale: 2.371   noise: 11.814\n",
      "Iter 1001/2000 - Loss: 3.254   lengthscale: 2.362   noise: 11.863\n",
      "Iter 1011/2000 - Loss: 3.248   lengthscale: 2.355   noise: 11.911\n",
      "Iter 1021/2000 - Loss: 3.247   lengthscale: 2.347   noise: 11.959\n",
      "Iter 1031/2000 - Loss: 3.244   lengthscale: 2.340   noise: 12.007\n",
      "Iter 1041/2000 - Loss: 3.240   lengthscale: 2.332   noise: 12.054\n",
      "Iter 1051/2000 - Loss: 3.238   lengthscale: 2.325   noise: 12.100\n",
      "Iter 1061/2000 - Loss: 3.236   lengthscale: 2.317   noise: 12.147\n",
      "Iter 1071/2000 - Loss: 3.233   lengthscale: 2.310   noise: 12.192\n",
      "Iter 1081/2000 - Loss: 3.232   lengthscale: 2.303   noise: 12.238\n",
      "Iter 1091/2000 - Loss: 3.229   lengthscale: 2.297   noise: 12.282\n",
      "Iter 1101/2000 - Loss: 3.224   lengthscale: 2.291   noise: 12.327\n",
      "Iter 1111/2000 - Loss: 3.224   lengthscale: 2.285   noise: 12.373\n",
      "Iter 1121/2000 - Loss: 3.220   lengthscale: 2.279   noise: 12.417\n",
      "Iter 1131/2000 - Loss: 3.218   lengthscale: 2.273   noise: 12.461\n",
      "Iter 1141/2000 - Loss: 3.214   lengthscale: 2.268   noise: 12.505\n",
      "Iter 1151/2000 - Loss: 3.209   lengthscale: 2.264   noise: 12.549\n",
      "Iter 1161/2000 - Loss: 3.212   lengthscale: 2.259   noise: 12.592\n",
      "Iter 1171/2000 - Loss: 3.208   lengthscale: 2.254   noise: 12.635\n",
      "Iter 1181/2000 - Loss: 3.206   lengthscale: 2.250   noise: 12.678\n",
      "Iter 1191/2000 - Loss: 3.205   lengthscale: 2.245   noise: 12.721\n",
      "Iter 1201/2000 - Loss: 3.201   lengthscale: 2.240   noise: 12.763\n",
      "Iter 1211/2000 - Loss: 3.198   lengthscale: 2.235   noise: 12.806\n",
      "Iter 1221/2000 - Loss: 3.199   lengthscale: 2.230   noise: 12.847\n",
      "Iter 1231/2000 - Loss: 3.196   lengthscale: 2.226   noise: 12.889\n",
      "Iter 1241/2000 - Loss: 3.198   lengthscale: 2.222   noise: 12.930\n",
      "Iter 1251/2000 - Loss: 3.192   lengthscale: 2.218   noise: 12.972\n",
      "Iter 1261/2000 - Loss: 3.190   lengthscale: 2.215   noise: 13.013\n",
      "Iter 1271/2000 - Loss: 3.190   lengthscale: 2.211   noise: 13.053\n",
      "Iter 1281/2000 - Loss: 3.189   lengthscale: 2.208   noise: 13.094\n",
      "Iter 1291/2000 - Loss: 3.188   lengthscale: 2.204   noise: 13.134\n",
      "Iter 1301/2000 - Loss: 3.181   lengthscale: 2.201   noise: 13.174\n",
      "Iter 1311/2000 - Loss: 3.178   lengthscale: 2.198   noise: 13.214\n",
      "Iter 1321/2000 - Loss: 3.180   lengthscale: 2.195   noise: 13.253\n",
      "Iter 1331/2000 - Loss: 3.177   lengthscale: 2.192   noise: 13.293\n",
      "Iter 1341/2000 - Loss: 3.175   lengthscale: 2.189   noise: 13.333\n",
      "Iter 1351/2000 - Loss: 3.174   lengthscale: 2.187   noise: 13.372\n",
      "Iter 1361/2000 - Loss: 3.174   lengthscale: 2.184   noise: 13.410\n",
      "Iter 1371/2000 - Loss: 3.172   lengthscale: 2.182   noise: 13.449\n",
      "Iter 1381/2000 - Loss: 3.170   lengthscale: 2.179   noise: 13.487\n",
      "Iter 1391/2000 - Loss: 3.169   lengthscale: 2.177   noise: 13.526\n",
      "Iter 1401/2000 - Loss: 3.169   lengthscale: 2.175   noise: 13.565\n",
      "Iter 1411/2000 - Loss: 3.171   lengthscale: 2.173   noise: 13.603\n",
      "Iter 1421/2000 - Loss: 3.164   lengthscale: 2.171   noise: 13.641\n",
      "Iter 1431/2000 - Loss: 3.166   lengthscale: 2.170   noise: 13.679\n",
      "Iter 1441/2000 - Loss: 3.164   lengthscale: 2.168   noise: 13.716\n",
      "Iter 1451/2000 - Loss: 3.161   lengthscale: 2.167   noise: 13.753\n",
      "Iter 1461/2000 - Loss: 3.163   lengthscale: 2.166   noise: 13.790\n",
      "Iter 1471/2000 - Loss: 3.158   lengthscale: 2.164   noise: 13.827\n",
      "Iter 1481/2000 - Loss: 3.156   lengthscale: 2.162   noise: 13.864\n",
      "Iter 1491/2000 - Loss: 3.160   lengthscale: 2.161   noise: 13.900\n",
      "Iter 1501/2000 - Loss: 3.157   lengthscale: 2.161   noise: 13.937\n",
      "Iter 1511/2000 - Loss: 3.154   lengthscale: 2.160   noise: 13.973\n",
      "Iter 1521/2000 - Loss: 3.158   lengthscale: 2.159   noise: 14.010\n",
      "Iter 1531/2000 - Loss: 3.156   lengthscale: 2.158   noise: 14.046\n",
      "Iter 1541/2000 - Loss: 3.151   lengthscale: 2.158   noise: 14.082\n",
      "Iter 1551/2000 - Loss: 3.153   lengthscale: 2.158   noise: 14.117\n",
      "Iter 1561/2000 - Loss: 3.149   lengthscale: 2.157   noise: 14.152\n",
      "Iter 1571/2000 - Loss: 3.149   lengthscale: 2.156   noise: 14.188\n",
      "Iter 1581/2000 - Loss: 3.146   lengthscale: 2.155   noise: 14.223\n",
      "Iter 1591/2000 - Loss: 3.149   lengthscale: 2.154   noise: 14.257\n",
      "Iter 1601/2000 - Loss: 3.149   lengthscale: 2.156   noise: 14.292\n",
      "Iter 1611/2000 - Loss: 3.145   lengthscale: 2.156   noise: 14.326\n",
      "Iter 1621/2000 - Loss: 3.144   lengthscale: 2.155   noise: 14.360\n",
      "Iter 1631/2000 - Loss: 3.141   lengthscale: 2.155   noise: 14.393\n",
      "Iter 1641/2000 - Loss: 3.141   lengthscale: 2.155   noise: 14.427\n",
      "Iter 1651/2000 - Loss: 3.142   lengthscale: 2.154   noise: 14.460\n",
      "Iter 1661/2000 - Loss: 3.142   lengthscale: 2.154   noise: 14.494\n",
      "Iter 1671/2000 - Loss: 3.140   lengthscale: 2.154   noise: 14.526\n",
      "Iter 1681/2000 - Loss: 3.139   lengthscale: 2.155   noise: 14.560\n",
      "Iter 1691/2000 - Loss: 3.134   lengthscale: 2.155   noise: 14.594\n",
      "Iter 1701/2000 - Loss: 3.135   lengthscale: 2.155   noise: 14.627\n",
      "Iter 1711/2000 - Loss: 3.136   lengthscale: 2.155   noise: 14.659\n",
      "Iter 1721/2000 - Loss: 3.137   lengthscale: 2.154   noise: 14.692\n",
      "Iter 1731/2000 - Loss: 3.133   lengthscale: 2.155   noise: 14.724\n",
      "Iter 1741/2000 - Loss: 3.132   lengthscale: 2.156   noise: 14.757\n",
      "Iter 1751/2000 - Loss: 3.128   lengthscale: 2.156   noise: 14.788\n",
      "Iter 1761/2000 - Loss: 3.130   lengthscale: 2.157   noise: 14.819\n",
      "Iter 1771/2000 - Loss: 3.129   lengthscale: 2.158   noise: 14.851\n",
      "Iter 1781/2000 - Loss: 3.128   lengthscale: 2.159   noise: 14.882\n",
      "Iter 1791/2000 - Loss: 3.130   lengthscale: 2.159   noise: 14.913\n",
      "Iter 1801/2000 - Loss: 3.131   lengthscale: 2.160   noise: 14.944\n",
      "Iter 1811/2000 - Loss: 3.123   lengthscale: 2.161   noise: 14.975\n",
      "Iter 1821/2000 - Loss: 3.126   lengthscale: 2.162   noise: 15.006\n",
      "Iter 1831/2000 - Loss: 3.129   lengthscale: 2.163   noise: 15.037\n",
      "Iter 1841/2000 - Loss: 3.128   lengthscale: 2.164   noise: 15.067\n",
      "Iter 1851/2000 - Loss: 3.122   lengthscale: 2.165   noise: 15.099\n",
      "Iter 1861/2000 - Loss: 3.127   lengthscale: 2.166   noise: 15.130\n",
      "Iter 1871/2000 - Loss: 3.124   lengthscale: 2.166   noise: 15.161\n",
      "Iter 1881/2000 - Loss: 3.123   lengthscale: 2.166   noise: 15.192\n",
      "Iter 1891/2000 - Loss: 3.127   lengthscale: 2.167   noise: 15.222\n",
      "Iter 1901/2000 - Loss: 3.119   lengthscale: 2.168   noise: 15.252\n",
      "Iter 1911/2000 - Loss: 3.119   lengthscale: 2.169   noise: 15.282\n",
      "Iter 1921/2000 - Loss: 3.120   lengthscale: 2.169   noise: 15.313\n",
      "Iter 1931/2000 - Loss: 3.120   lengthscale: 2.169   noise: 15.343\n",
      "Iter 1941/2000 - Loss: 3.121   lengthscale: 2.171   noise: 15.372\n",
      "Iter 1951/2000 - Loss: 3.117   lengthscale: 2.174   noise: 15.402\n",
      "Iter 1961/2000 - Loss: 3.118   lengthscale: 2.174   noise: 15.431\n",
      "Iter 1971/2000 - Loss: 3.115   lengthscale: 2.175   noise: 15.461\n",
      "Iter 1981/2000 - Loss: 3.113   lengthscale: 2.175   noise: 15.490\n",
      "Iter 1991/2000 - Loss: 3.116   lengthscale: 2.176   noise: 15.520\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 2000\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(X_train)\n",
    "\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, Y_train)\n",
    "    loss.backward()\n",
    "    if i%10 == 0 :\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictions\n",
    "\n",
    "We predict the selling price as well as a 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 10, 51)\n",
    "    Y_pred_dist = likelihood(model(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = Y_pred_dist.mean.cpu().numpy()\n",
    "conf_lo, conf_hi = Y_pred_dist.confidence_region()\n",
    "conf_lo, conf_hi = conf_lo.cpu(), conf_hi.cpu()\n",
    "\n",
    "X_test, Y_test = X_test.cpu(), Y_test.cpu()\n",
    "X_train, Y_train = X_train.cpu(), Y_train.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>conf_lo</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf_hi</th>\n",
       "      <th>inside_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.6</td>\n",
       "      <td>35.7</td>\n",
       "      <td>43.8</td>\n",
       "      <td>51.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.9</td>\n",
       "      <td>22.3</td>\n",
       "      <td>30.3</td>\n",
       "      <td>38.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.6</td>\n",
       "      <td>17.1</td>\n",
       "      <td>25.1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.2</td>\n",
       "      <td>25.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>21.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>29.8</td>\n",
       "      <td>37.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>27.1</td>\n",
       "      <td>35.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.8</td>\n",
       "      <td>16.7</td>\n",
       "      <td>24.7</td>\n",
       "      <td>32.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23.2</td>\n",
       "      <td>11.6</td>\n",
       "      <td>19.6</td>\n",
       "      <td>27.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>32.3</td>\n",
       "      <td>40.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>22.4</td>\n",
       "      <td>30.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>30.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36.1</td>\n",
       "      <td>22.6</td>\n",
       "      <td>30.6</td>\n",
       "      <td>38.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24.4</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17.2</td>\n",
       "      <td>12.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>14.6</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19.6</td>\n",
       "      <td>12.1</td>\n",
       "      <td>20.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>21.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>21.3</td>\n",
       "      <td>29.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>12.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>29.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.1</td>\n",
       "      <td>15.1</td>\n",
       "      <td>23.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32.2</td>\n",
       "      <td>21.6</td>\n",
       "      <td>29.6</td>\n",
       "      <td>37.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.7</td>\n",
       "      <td>20.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23.1</td>\n",
       "      <td>16.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>32.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>20.7</td>\n",
       "      <td>28.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>22.2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24.1</td>\n",
       "      <td>19.2</td>\n",
       "      <td>27.2</td>\n",
       "      <td>35.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>19.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>17.6</td>\n",
       "      <td>25.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>27.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>14.8</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>27.9</td>\n",
       "      <td>11.9</td>\n",
       "      <td>20.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>30.1</td>\n",
       "      <td>23.8</td>\n",
       "      <td>31.8</td>\n",
       "      <td>39.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>17.4</td>\n",
       "      <td>11.6</td>\n",
       "      <td>19.6</td>\n",
       "      <td>27.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>15.4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>31.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>32.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>14.2</td>\n",
       "      <td>9.8</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>19.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>20.6</td>\n",
       "      <td>28.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>50.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>44.2</td>\n",
       "      <td>52.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>21.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.7</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>11.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>19.4</td>\n",
       "      <td>13.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>29.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>13.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>17.5</td>\n",
       "      <td>10.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>9.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>20.3</td>\n",
       "      <td>9.8</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>18.6</td>\n",
       "      <td>13.8</td>\n",
       "      <td>21.8</td>\n",
       "      <td>29.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>50.0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>39.2</td>\n",
       "      <td>47.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>19.6</td>\n",
       "      <td>12.1</td>\n",
       "      <td>20.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>21.4</td>\n",
       "      <td>12.5</td>\n",
       "      <td>20.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>18.4</td>\n",
       "      <td>8.7</td>\n",
       "      <td>16.7</td>\n",
       "      <td>24.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>22.6</td>\n",
       "      <td>16.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>32.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>25.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>27.3</td>\n",
       "      <td>35.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>15.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>24.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>26.6</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>22.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>17.6</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>13.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>23.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>24.5</td>\n",
       "      <td>16.6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>32.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     truth  conf_lo  pred  conf_hi  inside_interval\n",
       "0     37.6     35.7  43.8     51.9                1\n",
       "1     27.9     22.3  30.3     38.3                1\n",
       "2     22.6     17.1  25.1     33.0                1\n",
       "3     13.8      5.5  15.0     24.4                1\n",
       "4     35.2     25.8  33.9     42.0                1\n",
       "5     10.4      5.3  13.5     21.8                1\n",
       "6     23.9     21.8  29.8     37.7                1\n",
       "7     29.0     19.1  27.1     35.1                1\n",
       "8     22.8     16.7  24.7     32.7                1\n",
       "9     23.2     11.6  19.6     27.6                1\n",
       "10    33.2     24.2  32.3     40.3                1\n",
       "11    19.0     14.4  22.4     30.3                1\n",
       "12    20.3     14.1  22.1     30.1                1\n",
       "13    36.1     22.6  30.6     38.6                1\n",
       "14    24.4     17.8  25.7     33.7                1\n",
       "15    17.2     12.3  20.3     28.3                1\n",
       "16    17.9      5.1  14.6     24.0                1\n",
       "17    19.6     12.1  20.2     28.2                1\n",
       "18    19.7      5.3  13.5     21.8                1\n",
       "19    15.0     13.3  21.3     29.2                1\n",
       "20     8.1      4.5  12.7     21.0                1\n",
       "21    23.0     13.9  21.9     29.9                1\n",
       "22    44.8     33.9  42.0     50.1                1\n",
       "23    23.1     15.1  23.1     31.1                1\n",
       "24    32.2     21.6  29.6     37.6                1\n",
       "25    10.8      4.6  12.7     20.9                1\n",
       "26    23.1     16.3  24.3     32.2                1\n",
       "27    21.2     12.7  20.7     28.7                1\n",
       "28    22.2     17.0  25.0     33.0                1\n",
       "29    24.1     19.2  27.2     35.2                1\n",
       "..     ...      ...   ...      ...              ...\n",
       "72    19.4      9.6  17.6     25.7                1\n",
       "73    27.5      6.6  14.8     23.0                0\n",
       "74    27.9     11.9  20.0     28.0                1\n",
       "75    30.1     23.8  31.8     39.8                1\n",
       "76    17.4     11.6  19.6     27.6                1\n",
       "77    15.4      4.9  13.0     21.2                1\n",
       "78    31.0     15.8  24.1     32.4                1\n",
       "79    14.2      9.8  17.8     25.8                1\n",
       "80    19.6     12.6  20.6     28.5                1\n",
       "81    50.0     36.0  44.2     52.4                1\n",
       "82    21.7      4.6  12.7     20.8                0\n",
       "83    11.7     10.5  18.5     26.5                1\n",
       "84    19.4     13.2  21.2     29.2                1\n",
       "85    13.0      7.3  15.3     23.4                1\n",
       "86    17.5     10.9  19.0     27.0                1\n",
       "87     9.7      5.6  14.0     22.5                1\n",
       "88    20.3      9.8  17.8     25.8                1\n",
       "89    18.6     13.8  21.8     29.8                1\n",
       "90    50.0     31.2  39.2     47.3                0\n",
       "91    19.6     12.1  20.1     28.1                1\n",
       "92    21.4     12.5  20.5     28.5                1\n",
       "93    18.4      8.7  16.7     24.7                1\n",
       "94    22.6     16.2  24.2     32.1                1\n",
       "95    25.0     19.4  27.3     35.3                1\n",
       "96    15.6      8.9  16.9     24.9                1\n",
       "97    26.6     21.0  29.0     37.0                1\n",
       "98    22.4      9.6  17.6     25.6                1\n",
       "99    13.1     10.0  18.1     26.3                1\n",
       "100   23.0     14.1  22.1     30.0                1\n",
       "101   24.5     16.6  24.6     32.5                1\n",
       "\n",
       "[102 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.DataFrame({'truth': Y_test.cpu(), \n",
    "                      'conf_lo': np.round(conf_lo, 1),\n",
    "                      'pred': np.round(Y_pred, 1), \n",
    "                      'conf_hi': np.round(conf_hi, 1)})\n",
    "preds['inside_interval'] = preds.apply(lambda row: 1 if ((row['truth'] >= row['conf_lo']) & (row['truth'] <= row['conf_hi'])) else 0, axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for testing set\n",
      "--------------------------------------\n",
      "RMSE is 3.9947081397879\n",
      "R2 score is 0.7961820844190546\n",
      "96 / 102 predictions are inside the interval\n"
     ]
    }
   ],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(Y_test, Y_pred)))\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"The model performance for testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print('{} / {} predictions are inside the interval'.format(sum(preds['inside_interval']), len(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest Regression\n",
    "\n",
    "as a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(max_depth=5, random_state=0,\n",
    "                             n_estimators=500)\n",
    "rfr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_rfr = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for testing set\n",
      "--------------------------------------\n",
      "RMSE is 3.8874798355550992\n",
      "R2 score is 0.8069772292270325\n"
     ]
    }
   ],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(Y_test, Y_pred_rfr)))\n",
    "r2 = r2_score(Y_test, Y_pred_rfr)\n",
    "\n",
    "print(\"The model performance for testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.6</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.9</td>\n",
       "      <td>27.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.6</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.8</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.2</td>\n",
       "      <td>41.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.4</td>\n",
       "      <td>13.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.9</td>\n",
       "      <td>29.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29.0</td>\n",
       "      <td>25.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23.2</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33.2</td>\n",
       "      <td>34.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19.0</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.3</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36.1</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24.4</td>\n",
       "      <td>24.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17.2</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.9</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19.6</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.7</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.0</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.1</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23.0</td>\n",
       "      <td>21.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44.8</td>\n",
       "      <td>45.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.1</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32.2</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.8</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23.1</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21.2</td>\n",
       "      <td>20.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>22.2</td>\n",
       "      <td>24.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24.1</td>\n",
       "      <td>26.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>19.4</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>27.5</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>27.9</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>30.1</td>\n",
       "      <td>29.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>17.4</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>15.4</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>31.0</td>\n",
       "      <td>26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>14.2</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>19.6</td>\n",
       "      <td>20.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>50.0</td>\n",
       "      <td>45.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>21.7</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>11.7</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>19.4</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>17.5</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>9.7</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>20.3</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>18.6</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>50.0</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>19.6</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>21.4</td>\n",
       "      <td>20.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>18.4</td>\n",
       "      <td>16.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>22.6</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>15.6</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>26.6</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>22.4</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>13.1</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>23.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>24.5</td>\n",
       "      <td>27.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     truth  pred\n",
       "0     37.6  47.3\n",
       "1     27.9  27.4\n",
       "2     22.6  23.8\n",
       "3     13.8  10.2\n",
       "4     35.2  41.8\n",
       "5     10.4  13.8\n",
       "6     23.9  29.8\n",
       "7     29.0  25.7\n",
       "8     22.8  23.1\n",
       "9     23.2  19.7\n",
       "10    33.2  34.1\n",
       "11    19.0  22.5\n",
       "12    20.3  21.5\n",
       "13    36.1  33.3\n",
       "14    24.4  24.3\n",
       "15    17.2  20.6\n",
       "16    17.9  10.2\n",
       "17    19.6  20.2\n",
       "18    19.7  14.3\n",
       "19    15.0  21.2\n",
       "20     8.1  15.4\n",
       "21    23.0  21.1\n",
       "22    44.8  45.2\n",
       "23    23.1  21.0\n",
       "24    32.2  30.2\n",
       "25    10.8  11.7\n",
       "26    23.1  24.0\n",
       "27    21.2  20.9\n",
       "28    22.2  24.2\n",
       "29    24.1  26.3\n",
       "..     ...   ...\n",
       "72    19.4  17.2\n",
       "73    27.5  14.2\n",
       "74    27.9  20.2\n",
       "75    30.1  29.5\n",
       "76    17.4  19.2\n",
       "77    15.4  13.4\n",
       "78    31.0  26.4\n",
       "79    14.2  17.7\n",
       "80    19.6  20.9\n",
       "81    50.0  45.8\n",
       "82    21.7  12.0\n",
       "83    11.7  17.2\n",
       "84    19.4  20.4\n",
       "85    13.0  15.0\n",
       "86    17.5  17.4\n",
       "87     9.7  13.5\n",
       "88    20.3  17.9\n",
       "89    18.6  22.5\n",
       "90    50.0  44.9\n",
       "91    19.6  20.8\n",
       "92    21.4  20.9\n",
       "93    18.4  16.2\n",
       "94    22.6  24.0\n",
       "95    25.0  25.1\n",
       "96    15.6  17.4\n",
       "97    26.6  29.0\n",
       "98    22.4  17.9\n",
       "99    13.1  16.7\n",
       "100   23.0  21.0\n",
       "101   24.5  27.4\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.DataFrame({'truth': Y_test, \n",
    "                      'pred': np.round(Y_pred_rfr, 1)})\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-env] *",
   "language": "python",
   "name": "conda-env-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
