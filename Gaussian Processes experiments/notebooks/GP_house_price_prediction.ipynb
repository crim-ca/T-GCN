{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP regression house price prediction\n",
    "\n",
    "\n",
    "Objectif: Utiliser une regresison Gaussienne pour prédire le prix d'une maison. On peut également obtenir un intervale de confiance à 2 sigma pour la prediction.\n",
    "\n",
    "\n",
    "Utilisation du Dataset UCI Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CRIM: Per capita crime rate by town\n",
    "* ZN: Proportion of residential land zoned for lots over 25,000 sq. ft\n",
    "* INDUS: Proportion of non-retail business acres per town\n",
    "* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "* NOX: Nitric oxide concentration (parts per 10 million)\n",
    "* RM: Average number of rooms per dwelling\n",
    "* AGE: Proportion of owner-occupied units built prior to 1940\n",
    "* DIS: Weighted distances to five Boston employment centers\n",
    "* RAD: Index of accessibility to radial highways\n",
    "* TAX: Full-value property tax rate per \\$10,000\n",
    "* PTRATIO: Pupil-teacher ratio by town\n",
    "* $ B: 1000(Bk — 0.63)^2 $, where Bk is the proportion of [people of African American descent] by town\n",
    "* LSTAT: Percentage of lower status of the population\n",
    "* MEDV: Median value of owner-occupied homes in $1000s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "We use the UCI boston hosuing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "boston = pd.DataFrame(load_boston().data, columns=load_boston().feature_names)\n",
    "boston['MEDV'] = load_boston().target\n",
    "boston.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([404, 2])\n",
      "torch.Size([102, 2])\n",
      "torch.Size([404])\n",
      "torch.Size([102])\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(np.c_[boston['LSTAT'], boston['RM']], columns = ['LSTAT','RM'])\n",
    "Y = boston['MEDV']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
    "X_train = torch.tensor(X_train.values).to(device)\n",
    "Y_train = torch.tensor(Y_train.values).to(device)\n",
    "X_test = torch.tensor(X_test.values).to(device)\n",
    "Y_test = torch.tensor(Y_test.values).to(device)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "We use a GP woth RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().double().to(device)\n",
    "model = ExactGPModel(X_train, Y_train, likelihood).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/2000 - Loss: 40.320   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/2000 - Loss: 15.567   lengthscale: 1.245   noise: 1.278\n",
      "Iter 21/2000 - Loss: 9.985   lengthscale: 1.700   noise: 1.888\n",
      "Iter 31/2000 - Loss: 7.998   lengthscale: 1.991   noise: 2.403\n",
      "Iter 41/2000 - Loss: 7.037   lengthscale: 2.171   noise: 2.814\n",
      "Iter 51/2000 - Loss: 6.479   lengthscale: 2.289   noise: 3.148\n",
      "Iter 61/2000 - Loss: 6.097   lengthscale: 2.373   noise: 3.431\n",
      "Iter 71/2000 - Loss: 5.812   lengthscale: 2.439   noise: 3.680\n",
      "Iter 81/2000 - Loss: 5.587   lengthscale: 2.494   noise: 3.907\n",
      "Iter 91/2000 - Loss: 5.401   lengthscale: 2.541   noise: 4.117\n",
      "Iter 101/2000 - Loss: 5.241   lengthscale: 2.584   noise: 4.314\n",
      "Iter 111/2000 - Loss: 5.104   lengthscale: 2.622   noise: 4.500\n",
      "Iter 121/2000 - Loss: 4.982   lengthscale: 2.657   noise: 4.678\n",
      "Iter 131/2000 - Loss: 4.873   lengthscale: 2.688   noise: 4.848\n",
      "Iter 141/2000 - Loss: 4.777   lengthscale: 2.717   noise: 5.011\n",
      "Iter 151/2000 - Loss: 4.692   lengthscale: 2.743   noise: 5.168\n",
      "Iter 161/2000 - Loss: 4.615   lengthscale: 2.767   noise: 5.319\n",
      "Iter 171/2000 - Loss: 4.542   lengthscale: 2.789   noise: 5.465\n",
      "Iter 181/2000 - Loss: 4.477   lengthscale: 2.808   noise: 5.607\n",
      "Iter 191/2000 - Loss: 4.418   lengthscale: 2.826   noise: 5.745\n",
      "Iter 201/2000 - Loss: 4.361   lengthscale: 2.842   noise: 5.879\n",
      "Iter 211/2000 - Loss: 4.308   lengthscale: 2.857   noise: 6.009\n",
      "Iter 221/2000 - Loss: 4.263   lengthscale: 2.871   noise: 6.136\n",
      "Iter 231/2000 - Loss: 4.215   lengthscale: 2.883   noise: 6.260\n",
      "Iter 241/2000 - Loss: 4.174   lengthscale: 2.894   noise: 6.381\n",
      "Iter 251/2000 - Loss: 4.136   lengthscale: 2.903   noise: 6.499\n",
      "Iter 261/2000 - Loss: 4.098   lengthscale: 2.911   noise: 6.615\n",
      "Iter 271/2000 - Loss: 4.063   lengthscale: 2.919   noise: 6.728\n",
      "Iter 281/2000 - Loss: 4.030   lengthscale: 2.925   noise: 6.839\n",
      "Iter 291/2000 - Loss: 3.999   lengthscale: 2.931   noise: 6.947\n",
      "Iter 301/2000 - Loss: 3.971   lengthscale: 2.935   noise: 7.053\n",
      "Iter 311/2000 - Loss: 3.942   lengthscale: 2.939   noise: 7.157\n",
      "Iter 321/2000 - Loss: 3.916   lengthscale: 2.942   noise: 7.259\n",
      "Iter 331/2000 - Loss: 3.890   lengthscale: 2.944   noise: 7.360\n",
      "Iter 341/2000 - Loss: 3.865   lengthscale: 2.945   noise: 7.459\n",
      "Iter 351/2000 - Loss: 3.844   lengthscale: 2.945   noise: 7.556\n",
      "Iter 361/2000 - Loss: 3.821   lengthscale: 2.945   noise: 7.651\n",
      "Iter 371/2000 - Loss: 3.800   lengthscale: 2.944   noise: 7.745\n",
      "Iter 381/2000 - Loss: 3.780   lengthscale: 2.943   noise: 7.837\n",
      "Iter 391/2000 - Loss: 3.760   lengthscale: 2.941   noise: 7.928\n",
      "Iter 401/2000 - Loss: 3.741   lengthscale: 2.938   noise: 8.017\n",
      "Iter 411/2000 - Loss: 3.723   lengthscale: 2.935   noise: 8.106\n",
      "Iter 421/2000 - Loss: 3.705   lengthscale: 2.932   noise: 8.192\n",
      "Iter 431/2000 - Loss: 3.691   lengthscale: 2.928   noise: 8.278\n",
      "Iter 441/2000 - Loss: 3.674   lengthscale: 2.923   noise: 8.362\n",
      "Iter 451/2000 - Loss: 3.659   lengthscale: 2.917   noise: 8.446\n",
      "Iter 461/2000 - Loss: 3.644   lengthscale: 2.911   noise: 8.528\n",
      "Iter 471/2000 - Loss: 3.631   lengthscale: 2.905   noise: 8.610\n",
      "Iter 481/2000 - Loss: 3.616   lengthscale: 2.898   noise: 8.690\n",
      "Iter 491/2000 - Loss: 3.602   lengthscale: 2.891   noise: 8.769\n",
      "Iter 501/2000 - Loss: 3.590   lengthscale: 2.884   noise: 8.847\n",
      "Iter 511/2000 - Loss: 3.577   lengthscale: 2.876   noise: 8.924\n",
      "Iter 521/2000 - Loss: 3.564   lengthscale: 2.868   noise: 9.000\n",
      "Iter 531/2000 - Loss: 3.553   lengthscale: 2.859   noise: 9.075\n",
      "Iter 541/2000 - Loss: 3.543   lengthscale: 2.850   noise: 9.149\n",
      "Iter 551/2000 - Loss: 3.531   lengthscale: 2.841   noise: 9.222\n",
      "Iter 561/2000 - Loss: 3.518   lengthscale: 2.832   noise: 9.295\n",
      "Iter 571/2000 - Loss: 3.510   lengthscale: 2.822   noise: 9.366\n",
      "Iter 581/2000 - Loss: 3.502   lengthscale: 2.812   noise: 9.437\n",
      "Iter 591/2000 - Loss: 3.490   lengthscale: 2.802   noise: 9.507\n",
      "Iter 601/2000 - Loss: 3.481   lengthscale: 2.791   noise: 9.576\n",
      "Iter 611/2000 - Loss: 3.474   lengthscale: 2.781   noise: 9.645\n",
      "Iter 621/2000 - Loss: 3.462   lengthscale: 2.770   noise: 9.713\n",
      "Iter 631/2000 - Loss: 3.455   lengthscale: 2.759   noise: 9.779\n",
      "Iter 641/2000 - Loss: 3.444   lengthscale: 2.748   noise: 9.845\n",
      "Iter 651/2000 - Loss: 3.440   lengthscale: 2.737   noise: 9.911\n",
      "Iter 661/2000 - Loss: 3.429   lengthscale: 2.725   noise: 9.976\n",
      "Iter 671/2000 - Loss: 3.421   lengthscale: 2.714   noise: 10.040\n",
      "Iter 681/2000 - Loss: 3.414   lengthscale: 2.702   noise: 10.103\n",
      "Iter 691/2000 - Loss: 3.408   lengthscale: 2.690   noise: 10.166\n",
      "Iter 701/2000 - Loss: 3.399   lengthscale: 2.678   noise: 10.228\n",
      "Iter 711/2000 - Loss: 3.393   lengthscale: 2.666   noise: 10.290\n",
      "Iter 721/2000 - Loss: 3.385   lengthscale: 2.654   noise: 10.351\n",
      "Iter 731/2000 - Loss: 3.379   lengthscale: 2.643   noise: 10.412\n",
      "Iter 741/2000 - Loss: 3.372   lengthscale: 2.631   noise: 10.472\n",
      "Iter 751/2000 - Loss: 3.370   lengthscale: 2.618   noise: 10.532\n",
      "Iter 761/2000 - Loss: 3.362   lengthscale: 2.606   noise: 10.592\n",
      "Iter 771/2000 - Loss: 3.355   lengthscale: 2.595   noise: 10.650\n",
      "Iter 781/2000 - Loss: 3.348   lengthscale: 2.583   noise: 10.708\n",
      "Iter 791/2000 - Loss: 3.341   lengthscale: 2.571   noise: 10.766\n",
      "Iter 801/2000 - Loss: 3.337   lengthscale: 2.560   noise: 10.823\n",
      "Iter 811/2000 - Loss: 3.330   lengthscale: 2.548   noise: 10.879\n",
      "Iter 821/2000 - Loss: 3.328   lengthscale: 2.537   noise: 10.934\n",
      "Iter 831/2000 - Loss: 3.321   lengthscale: 2.526   noise: 10.990\n",
      "Iter 841/2000 - Loss: 3.317   lengthscale: 2.515   noise: 11.044\n",
      "Iter 851/2000 - Loss: 3.311   lengthscale: 2.505   noise: 11.098\n",
      "Iter 861/2000 - Loss: 3.307   lengthscale: 2.494   noise: 11.152\n",
      "Iter 871/2000 - Loss: 3.304   lengthscale: 2.484   noise: 11.205\n",
      "Iter 881/2000 - Loss: 3.299   lengthscale: 2.473   noise: 11.258\n",
      "Iter 891/2000 - Loss: 3.295   lengthscale: 2.463   noise: 11.311\n",
      "Iter 901/2000 - Loss: 3.290   lengthscale: 2.453   noise: 11.363\n",
      "Iter 911/2000 - Loss: 3.287   lengthscale: 2.442   noise: 11.415\n",
      "Iter 921/2000 - Loss: 3.282   lengthscale: 2.433   noise: 11.466\n",
      "Iter 931/2000 - Loss: 3.276   lengthscale: 2.424   noise: 11.517\n",
      "Iter 941/2000 - Loss: 3.274   lengthscale: 2.415   noise: 11.566\n",
      "Iter 951/2000 - Loss: 3.270   lengthscale: 2.406   noise: 11.616\n",
      "Iter 961/2000 - Loss: 3.266   lengthscale: 2.397   noise: 11.666\n",
      "Iter 971/2000 - Loss: 3.263   lengthscale: 2.388   noise: 11.715\n",
      "Iter 981/2000 - Loss: 3.258   lengthscale: 2.380   noise: 11.763\n",
      "Iter 991/2000 - Loss: 3.259   lengthscale: 2.372   noise: 11.812\n",
      "Iter 1001/2000 - Loss: 3.254   lengthscale: 2.365   noise: 11.860\n",
      "Iter 1011/2000 - Loss: 3.249   lengthscale: 2.357   noise: 11.908\n",
      "Iter 1021/2000 - Loss: 3.246   lengthscale: 2.349   noise: 11.956\n",
      "Iter 1031/2000 - Loss: 3.242   lengthscale: 2.341   noise: 12.003\n",
      "Iter 1041/2000 - Loss: 3.242   lengthscale: 2.333   noise: 12.050\n",
      "Iter 1051/2000 - Loss: 3.237   lengthscale: 2.326   noise: 12.097\n",
      "Iter 1061/2000 - Loss: 3.233   lengthscale: 2.319   noise: 12.143\n",
      "Iter 1071/2000 - Loss: 3.230   lengthscale: 2.312   noise: 12.188\n",
      "Iter 1081/2000 - Loss: 3.230   lengthscale: 2.305   noise: 12.233\n",
      "Iter 1091/2000 - Loss: 3.228   lengthscale: 2.298   noise: 12.278\n",
      "Iter 1101/2000 - Loss: 3.225   lengthscale: 2.291   noise: 12.323\n",
      "Iter 1111/2000 - Loss: 3.223   lengthscale: 2.284   noise: 12.367\n",
      "Iter 1121/2000 - Loss: 3.217   lengthscale: 2.278   noise: 12.412\n",
      "Iter 1131/2000 - Loss: 3.219   lengthscale: 2.271   noise: 12.456\n",
      "Iter 1141/2000 - Loss: 3.214   lengthscale: 2.266   noise: 12.500\n",
      "Iter 1151/2000 - Loss: 3.213   lengthscale: 2.260   noise: 12.544\n",
      "Iter 1161/2000 - Loss: 3.210   lengthscale: 2.255   noise: 12.587\n",
      "Iter 1171/2000 - Loss: 3.207   lengthscale: 2.250   noise: 12.631\n",
      "Iter 1181/2000 - Loss: 3.204   lengthscale: 2.245   noise: 12.674\n",
      "Iter 1191/2000 - Loss: 3.206   lengthscale: 2.241   noise: 12.717\n",
      "Iter 1201/2000 - Loss: 3.202   lengthscale: 2.237   noise: 12.759\n",
      "Iter 1211/2000 - Loss: 3.203   lengthscale: 2.233   noise: 12.801\n",
      "Iter 1221/2000 - Loss: 3.196   lengthscale: 2.229   noise: 12.843\n",
      "Iter 1231/2000 - Loss: 3.195   lengthscale: 2.225   noise: 12.885\n",
      "Iter 1241/2000 - Loss: 3.194   lengthscale: 2.221   noise: 12.927\n",
      "Iter 1251/2000 - Loss: 3.193   lengthscale: 2.217   noise: 12.968\n",
      "Iter 1261/2000 - Loss: 3.192   lengthscale: 2.213   noise: 13.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1271/2000 - Loss: 3.189   lengthscale: 2.210   noise: 13.049\n",
      "Iter 1281/2000 - Loss: 3.185   lengthscale: 2.207   noise: 13.090\n",
      "Iter 1291/2000 - Loss: 3.186   lengthscale: 2.204   noise: 13.131\n",
      "Iter 1301/2000 - Loss: 3.181   lengthscale: 2.200   noise: 13.171\n",
      "Iter 1311/2000 - Loss: 3.180   lengthscale: 2.196   noise: 13.211\n",
      "Iter 1321/2000 - Loss: 3.181   lengthscale: 2.192   noise: 13.251\n",
      "Iter 1331/2000 - Loss: 3.179   lengthscale: 2.189   noise: 13.291\n",
      "Iter 1341/2000 - Loss: 3.177   lengthscale: 2.187   noise: 13.330\n",
      "Iter 1351/2000 - Loss: 3.174   lengthscale: 2.184   noise: 13.369\n",
      "Iter 1361/2000 - Loss: 3.176   lengthscale: 2.182   noise: 13.408\n",
      "Iter 1371/2000 - Loss: 3.174   lengthscale: 2.180   noise: 13.446\n",
      "Iter 1381/2000 - Loss: 3.173   lengthscale: 2.178   noise: 13.484\n",
      "Iter 1391/2000 - Loss: 3.169   lengthscale: 2.177   noise: 13.522\n",
      "Iter 1401/2000 - Loss: 3.169   lengthscale: 2.175   noise: 13.560\n",
      "Iter 1411/2000 - Loss: 3.166   lengthscale: 2.174   noise: 13.597\n",
      "Iter 1421/2000 - Loss: 3.167   lengthscale: 2.173   noise: 13.635\n",
      "Iter 1431/2000 - Loss: 3.169   lengthscale: 2.172   noise: 13.672\n",
      "Iter 1441/2000 - Loss: 3.166   lengthscale: 2.171   noise: 13.709\n",
      "Iter 1451/2000 - Loss: 3.163   lengthscale: 2.170   noise: 13.745\n",
      "Iter 1461/2000 - Loss: 3.158   lengthscale: 2.169   noise: 13.782\n",
      "Iter 1471/2000 - Loss: 3.159   lengthscale: 2.167   noise: 13.818\n",
      "Iter 1481/2000 - Loss: 3.161   lengthscale: 2.166   noise: 13.855\n",
      "Iter 1491/2000 - Loss: 3.157   lengthscale: 2.164   noise: 13.891\n",
      "Iter 1501/2000 - Loss: 3.154   lengthscale: 2.163   noise: 13.928\n",
      "Iter 1511/2000 - Loss: 3.156   lengthscale: 2.161   noise: 13.965\n",
      "Iter 1521/2000 - Loss: 3.154   lengthscale: 2.161   noise: 14.000\n",
      "Iter 1531/2000 - Loss: 3.151   lengthscale: 2.161   noise: 14.036\n",
      "Iter 1541/2000 - Loss: 3.152   lengthscale: 2.160   noise: 14.072\n",
      "Iter 1551/2000 - Loss: 3.151   lengthscale: 2.159   noise: 14.107\n",
      "Iter 1561/2000 - Loss: 3.149   lengthscale: 2.158   noise: 14.142\n",
      "Iter 1571/2000 - Loss: 3.147   lengthscale: 2.157   noise: 14.177\n",
      "Iter 1581/2000 - Loss: 3.147   lengthscale: 2.156   noise: 14.212\n",
      "Iter 1591/2000 - Loss: 3.147   lengthscale: 2.155   noise: 14.247\n",
      "Iter 1601/2000 - Loss: 3.144   lengthscale: 2.154   noise: 14.281\n",
      "Iter 1611/2000 - Loss: 3.146   lengthscale: 2.153   noise: 14.316\n",
      "Iter 1621/2000 - Loss: 3.145   lengthscale: 2.153   noise: 14.350\n",
      "Iter 1631/2000 - Loss: 3.143   lengthscale: 2.152   noise: 14.384\n",
      "Iter 1641/2000 - Loss: 3.141   lengthscale: 2.153   noise: 14.417\n",
      "Iter 1651/2000 - Loss: 3.145   lengthscale: 2.153   noise: 14.451\n",
      "Iter 1661/2000 - Loss: 3.139   lengthscale: 2.154   noise: 14.484\n",
      "Iter 1671/2000 - Loss: 3.138   lengthscale: 2.155   noise: 14.517\n",
      "Iter 1681/2000 - Loss: 3.138   lengthscale: 2.156   noise: 14.550\n",
      "Iter 1691/2000 - Loss: 3.136   lengthscale: 2.157   noise: 14.583\n",
      "Iter 1701/2000 - Loss: 3.136   lengthscale: 2.157   noise: 14.615\n",
      "Iter 1711/2000 - Loss: 3.135   lengthscale: 2.157   noise: 14.647\n",
      "Iter 1721/2000 - Loss: 3.135   lengthscale: 2.157   noise: 14.680\n",
      "Iter 1731/2000 - Loss: 3.137   lengthscale: 2.157   noise: 14.712\n",
      "Iter 1741/2000 - Loss: 3.134   lengthscale: 2.158   noise: 14.745\n",
      "Iter 1751/2000 - Loss: 3.132   lengthscale: 2.158   noise: 14.777\n",
      "Iter 1761/2000 - Loss: 3.132   lengthscale: 2.158   noise: 14.809\n",
      "Iter 1771/2000 - Loss: 3.131   lengthscale: 2.158   noise: 14.842\n",
      "Iter 1781/2000 - Loss: 3.128   lengthscale: 2.158   noise: 14.874\n",
      "Iter 1791/2000 - Loss: 3.128   lengthscale: 2.159   noise: 14.906\n",
      "Iter 1801/2000 - Loss: 3.128   lengthscale: 2.159   noise: 14.937\n",
      "Iter 1811/2000 - Loss: 3.126   lengthscale: 2.159   noise: 14.969\n",
      "Iter 1821/2000 - Loss: 3.128   lengthscale: 2.159   noise: 15.001\n",
      "Iter 1831/2000 - Loss: 3.126   lengthscale: 2.159   noise: 15.032\n",
      "Iter 1841/2000 - Loss: 3.125   lengthscale: 2.159   noise: 15.063\n",
      "Iter 1851/2000 - Loss: 3.125   lengthscale: 2.160   noise: 15.095\n",
      "Iter 1861/2000 - Loss: 3.125   lengthscale: 2.161   noise: 15.126\n",
      "Iter 1871/2000 - Loss: 3.125   lengthscale: 2.162   noise: 15.157\n",
      "Iter 1881/2000 - Loss: 3.124   lengthscale: 2.162   noise: 15.188\n",
      "Iter 1891/2000 - Loss: 3.122   lengthscale: 2.163   noise: 15.219\n",
      "Iter 1901/2000 - Loss: 3.122   lengthscale: 2.164   noise: 15.249\n",
      "Iter 1911/2000 - Loss: 3.120   lengthscale: 2.164   noise: 15.280\n",
      "Iter 1921/2000 - Loss: 3.120   lengthscale: 2.164   noise: 15.310\n",
      "Iter 1931/2000 - Loss: 3.118   lengthscale: 2.165   noise: 15.340\n",
      "Iter 1941/2000 - Loss: 3.116   lengthscale: 2.166   noise: 15.371\n",
      "Iter 1951/2000 - Loss: 3.117   lengthscale: 2.167   noise: 15.400\n",
      "Iter 1961/2000 - Loss: 3.118   lengthscale: 2.166   noise: 15.430\n",
      "Iter 1971/2000 - Loss: 3.115   lengthscale: 2.167   noise: 15.460\n",
      "Iter 1981/2000 - Loss: 3.118   lengthscale: 2.167   noise: 15.490\n",
      "Iter 1991/2000 - Loss: 3.116   lengthscale: 2.167   noise: 15.518\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 2000\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(X_train)\n",
    "\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, Y_train)\n",
    "    loss.backward()\n",
    "    if i%10 == 0 :\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictions\n",
    "\n",
    "We predict the selling price as well as a 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 10, 51)\n",
    "    Y_pred_dist = likelihood(model(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = Y_pred_dist.mean.cpu().numpy()\n",
    "conf_lo, conf_hi = Y_pred_dist.confidence_region()\n",
    "conf_lo, conf_hi = conf_lo.cpu(), conf_hi.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>conf_lo</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf_hi</th>\n",
       "      <th>inside_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.6</td>\n",
       "      <td>35.8</td>\n",
       "      <td>43.9</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.9</td>\n",
       "      <td>22.3</td>\n",
       "      <td>30.3</td>\n",
       "      <td>38.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.6</td>\n",
       "      <td>17.1</td>\n",
       "      <td>25.1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.2</td>\n",
       "      <td>25.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>21.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>29.8</td>\n",
       "      <td>37.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>27.1</td>\n",
       "      <td>35.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.8</td>\n",
       "      <td>16.7</td>\n",
       "      <td>24.7</td>\n",
       "      <td>32.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23.2</td>\n",
       "      <td>11.6</td>\n",
       "      <td>19.6</td>\n",
       "      <td>27.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>32.3</td>\n",
       "      <td>40.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>22.4</td>\n",
       "      <td>30.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>30.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36.1</td>\n",
       "      <td>22.6</td>\n",
       "      <td>30.6</td>\n",
       "      <td>38.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24.4</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17.2</td>\n",
       "      <td>12.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>28.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>14.6</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19.6</td>\n",
       "      <td>12.1</td>\n",
       "      <td>20.2</td>\n",
       "      <td>28.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>21.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>21.3</td>\n",
       "      <td>29.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>12.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>29.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.1</td>\n",
       "      <td>15.1</td>\n",
       "      <td>23.1</td>\n",
       "      <td>31.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32.2</td>\n",
       "      <td>21.6</td>\n",
       "      <td>29.6</td>\n",
       "      <td>37.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.7</td>\n",
       "      <td>20.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23.1</td>\n",
       "      <td>16.3</td>\n",
       "      <td>24.3</td>\n",
       "      <td>32.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21.2</td>\n",
       "      <td>12.7</td>\n",
       "      <td>20.7</td>\n",
       "      <td>28.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>22.2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24.1</td>\n",
       "      <td>19.2</td>\n",
       "      <td>27.2</td>\n",
       "      <td>35.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>19.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>17.6</td>\n",
       "      <td>25.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>27.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>14.8</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>27.9</td>\n",
       "      <td>11.9</td>\n",
       "      <td>20.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>30.1</td>\n",
       "      <td>23.8</td>\n",
       "      <td>31.8</td>\n",
       "      <td>39.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>17.4</td>\n",
       "      <td>11.6</td>\n",
       "      <td>19.6</td>\n",
       "      <td>27.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>15.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>31.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>32.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>14.2</td>\n",
       "      <td>9.8</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>19.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>20.6</td>\n",
       "      <td>28.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>50.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>44.2</td>\n",
       "      <td>52.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>21.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.7</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>11.7</td>\n",
       "      <td>10.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>26.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>19.4</td>\n",
       "      <td>13.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>29.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>13.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>17.5</td>\n",
       "      <td>10.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>9.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>20.3</td>\n",
       "      <td>9.8</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>18.6</td>\n",
       "      <td>13.8</td>\n",
       "      <td>21.8</td>\n",
       "      <td>29.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>50.0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>39.2</td>\n",
       "      <td>47.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>19.6</td>\n",
       "      <td>12.1</td>\n",
       "      <td>20.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>21.4</td>\n",
       "      <td>12.5</td>\n",
       "      <td>20.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>18.4</td>\n",
       "      <td>8.7</td>\n",
       "      <td>16.7</td>\n",
       "      <td>24.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>22.6</td>\n",
       "      <td>16.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>32.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>25.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>27.3</td>\n",
       "      <td>35.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>15.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>24.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>26.6</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>22.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>17.6</td>\n",
       "      <td>25.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>13.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>23.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>24.5</td>\n",
       "      <td>16.6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>32.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     truth  conf_lo  pred  conf_hi  inside_interval\n",
       "0     37.6     35.8  43.9     52.0                1\n",
       "1     27.9     22.3  30.3     38.3                1\n",
       "2     22.6     17.1  25.1     33.0                1\n",
       "3     13.8      5.5  15.0     24.4                1\n",
       "4     35.2     25.8  33.9     42.0                1\n",
       "5     10.4      5.3  13.5     21.8                1\n",
       "6     23.9     21.8  29.8     37.7                1\n",
       "7     29.0     19.1  27.1     35.1                1\n",
       "8     22.8     16.7  24.7     32.7                1\n",
       "9     23.2     11.6  19.6     27.6                1\n",
       "10    33.2     24.2  32.3     40.3                1\n",
       "11    19.0     14.4  22.4     30.3                1\n",
       "12    20.3     14.1  22.1     30.1                1\n",
       "13    36.1     22.6  30.6     38.7                1\n",
       "14    24.4     17.8  25.7     33.7                1\n",
       "15    17.2     12.3  20.3     28.3                1\n",
       "16    17.9      5.1  14.6     24.0                1\n",
       "17    19.6     12.1  20.2     28.2                1\n",
       "18    19.7      5.3  13.5     21.8                1\n",
       "19    15.0     13.3  21.3     29.3                1\n",
       "20     8.1      4.5  12.7     21.0                1\n",
       "21    23.0     13.9  21.9     29.9                1\n",
       "22    44.8     33.9  42.0     50.2                1\n",
       "23    23.1     15.1  23.1     31.1                1\n",
       "24    32.2     21.6  29.6     37.6                1\n",
       "25    10.8      4.6  12.7     20.9                1\n",
       "26    23.1     16.3  24.3     32.2                1\n",
       "27    21.2     12.7  20.7     28.7                1\n",
       "28    22.2     17.0  25.0     32.9                1\n",
       "29    24.1     19.2  27.2     35.2                1\n",
       "..     ...      ...   ...      ...              ...\n",
       "72    19.4      9.6  17.6     25.7                1\n",
       "73    27.5      6.6  14.8     23.0                0\n",
       "74    27.9     11.9  20.0     28.0                1\n",
       "75    30.1     23.8  31.8     39.8                1\n",
       "76    17.4     11.6  19.6     27.6                1\n",
       "77    15.4      4.8  13.0     21.2                1\n",
       "78    31.0     15.8  24.1     32.4                1\n",
       "79    14.2      9.8  17.8     25.8                1\n",
       "80    19.6     12.6  20.6     28.6                1\n",
       "81    50.0     36.0  44.2     52.4                1\n",
       "82    21.7      4.6  12.7     20.8                0\n",
       "83    11.7     10.5  18.5     26.4                1\n",
       "84    19.4     13.2  21.2     29.2                1\n",
       "85    13.0      7.3  15.3     23.4                1\n",
       "86    17.5     10.9  19.0     27.0                1\n",
       "87     9.7      5.6  14.0     22.5                1\n",
       "88    20.3      9.8  17.8     25.8                1\n",
       "89    18.6     13.8  21.8     29.8                1\n",
       "90    50.0     31.2  39.2     47.3                0\n",
       "91    19.6     12.1  20.1     28.1                1\n",
       "92    21.4     12.5  20.5     28.5                1\n",
       "93    18.4      8.7  16.7     24.7                1\n",
       "94    22.6     16.2  24.2     32.1                1\n",
       "95    25.0     19.3  27.3     35.3                1\n",
       "96    15.6      8.9  16.9     24.9                1\n",
       "97    26.6     21.0  29.0     37.0                1\n",
       "98    22.4      9.6  17.6     25.6                1\n",
       "99    13.1     10.0  18.1     26.3                1\n",
       "100   23.0     14.1  22.1     30.0                1\n",
       "101   24.5     16.6  24.6     32.5                1\n",
       "\n",
       "[102 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.DataFrame({'truth': Y_test, \n",
    "                      'conf_lo': np.round(conf_lo, 1),\n",
    "                      'pred': np.round(Y_pred, 1), \n",
    "                      'conf_hi': np.round(conf_hi, 1)})\n",
    "preds['inside_interval'] = preds.apply(lambda row: 1 if ((row['truth'] >= row['conf_lo']) & (row['truth'] <= row['conf_hi'])) else 0, axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for testing set\n",
      "--------------------------------------\n",
      "RMSE is 3.9946383888660817\n",
      "R2 score is 0.7961892020170777\n",
      "96 / 102 predictions are inside the interval\n"
     ]
    }
   ],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(Y_test, Y_pred)))\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"The model performance for testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print('{} / {} predictions are inside the interval'.format(sum(preds['inside_interval']), len(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest Regression\n",
    "\n",
    "as a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(max_depth=5, random_state=0,\n",
    "                             n_estimators=500)\n",
    "rfr.fit(X_train.cpu(), Y_train.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_rfr = rfr.predict(X_test.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for testing set\n",
      "--------------------------------------\n",
      "RMSE is 3.8874798355550992\n",
      "R2 score is 0.8069772292270325\n"
     ]
    }
   ],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(Y_test, Y_pred_rfr)))\n",
    "r2 = r2_score(Y_test, Y_pred_rfr)\n",
    "\n",
    "print(\"The model performance for testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.6</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.9</td>\n",
       "      <td>27.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.6</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.8</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.2</td>\n",
       "      <td>41.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.4</td>\n",
       "      <td>13.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.9</td>\n",
       "      <td>29.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29.0</td>\n",
       "      <td>25.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23.2</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33.2</td>\n",
       "      <td>34.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19.0</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.3</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36.1</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24.4</td>\n",
       "      <td>24.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17.2</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.9</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19.6</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.7</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.0</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.1</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23.0</td>\n",
       "      <td>21.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44.8</td>\n",
       "      <td>45.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.1</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32.2</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.8</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23.1</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21.2</td>\n",
       "      <td>20.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>22.2</td>\n",
       "      <td>24.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24.1</td>\n",
       "      <td>26.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>19.4</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>27.5</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>27.9</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>30.1</td>\n",
       "      <td>29.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>17.4</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>15.4</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>31.0</td>\n",
       "      <td>26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>14.2</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>19.6</td>\n",
       "      <td>20.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>50.0</td>\n",
       "      <td>45.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>21.7</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>11.7</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>19.4</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>17.5</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>9.7</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>20.3</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>18.6</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>50.0</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>19.6</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>21.4</td>\n",
       "      <td>20.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>18.4</td>\n",
       "      <td>16.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>22.6</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>15.6</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>26.6</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>22.4</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>13.1</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>23.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>24.5</td>\n",
       "      <td>27.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     truth  pred\n",
       "0     37.6  47.3\n",
       "1     27.9  27.4\n",
       "2     22.6  23.8\n",
       "3     13.8  10.2\n",
       "4     35.2  41.8\n",
       "5     10.4  13.8\n",
       "6     23.9  29.8\n",
       "7     29.0  25.7\n",
       "8     22.8  23.1\n",
       "9     23.2  19.7\n",
       "10    33.2  34.1\n",
       "11    19.0  22.5\n",
       "12    20.3  21.5\n",
       "13    36.1  33.3\n",
       "14    24.4  24.3\n",
       "15    17.2  20.6\n",
       "16    17.9  10.2\n",
       "17    19.6  20.2\n",
       "18    19.7  14.3\n",
       "19    15.0  21.2\n",
       "20     8.1  15.4\n",
       "21    23.0  21.1\n",
       "22    44.8  45.2\n",
       "23    23.1  21.0\n",
       "24    32.2  30.2\n",
       "25    10.8  11.7\n",
       "26    23.1  24.0\n",
       "27    21.2  20.9\n",
       "28    22.2  24.2\n",
       "29    24.1  26.3\n",
       "..     ...   ...\n",
       "72    19.4  17.2\n",
       "73    27.5  14.2\n",
       "74    27.9  20.2\n",
       "75    30.1  29.5\n",
       "76    17.4  19.2\n",
       "77    15.4  13.4\n",
       "78    31.0  26.4\n",
       "79    14.2  17.7\n",
       "80    19.6  20.9\n",
       "81    50.0  45.8\n",
       "82    21.7  12.0\n",
       "83    11.7  17.2\n",
       "84    19.4  20.4\n",
       "85    13.0  15.0\n",
       "86    17.5  17.4\n",
       "87     9.7  13.5\n",
       "88    20.3  17.9\n",
       "89    18.6  22.5\n",
       "90    50.0  44.9\n",
       "91    19.6  20.8\n",
       "92    21.4  20.9\n",
       "93    18.4  16.2\n",
       "94    22.6  24.0\n",
       "95    25.0  25.1\n",
       "96    15.6  17.4\n",
       "97    26.6  29.0\n",
       "98    22.4  17.9\n",
       "99    13.1  16.7\n",
       "100   23.0  21.0\n",
       "101   24.5  27.4\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.DataFrame({'truth': Y_test, \n",
    "                      'pred': np.round(Y_pred_rfr, 1)})\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-env] *",
   "language": "python",
   "name": "conda-env-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
